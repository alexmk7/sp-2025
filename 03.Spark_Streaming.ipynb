{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming \n",
    "\n",
    "`Structured Streaming` — это масштабируемая и отказоустойчивая библиотека для потоковой обработки, построенный на базе `Spark SQL`. Основная идея - с потоковыми вычислениями можно работать так же, как и со статическими данными. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import dbldatagen as dg\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, session_window\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    " # Если переменная окружения  `JAVA_HOME` не установлена, то тут можно её указать.\n",
    "os.environ[\"JAVA_HOME\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем сессию `Spark`, как обычно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/11/19 14:33:15 WARN Utils: Your hostname, burg, resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/11/19 14:33:15 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/19 14:33:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"structured\") \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", True) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем, создадим \"статический\" DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([(\"row1\", 10), (\"row2\", 200)], [\"column1\", \"columns2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель исполнения:\n",
    "1. Входные данные поступают пачками (`mini-batch`) и добавляются к некоторому \"бесконечному\" DataFrame. Размер и частота появления `mini-batch` зависит от источника (генерируются \"по триггеру\").\n",
    "2. Пользователем описываются некоторые операции по преобразованию \"бесконечного DataFrame\", как в \"статическом\" Spark.\n",
    "3. В итоге получается \"результирующий DataFrame\", который является результатом работы и записывается во внешний источник (топик Kafka, консоль, файлы, etc)\n",
    "\n",
    "Создаем Streaming DataFrame, описывая процесс получения данных из какого-нибудь источника. Поддерживается 4 встроенных источника:\n",
    "- Kafka (`kafka`)\n",
    "- Файлы \n",
    "- Сеть (`socket`)\n",
    "- Генерация DataFrame вида `(timestamp TIMESTAMP, value LONG )`, для тестовых целей (`rate`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Для Kafka нужно указать топик\n",
    "\n",
    "# df = spark \\\n",
    "#   .readStream \\\n",
    "#   .format(\"kafka\") \\\n",
    "#   .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "#   .option(\"subscribePattern\", \"topic*\") \\\n",
    "#   .option(\"startingOffsets\", \"earliest\") \\\n",
    "#   .load()\n",
    "\n",
    "\n",
    "# Будет создаваться 10 записей в секунду \n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", \"10\") \\\n",
    "    .load()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно запустить процесс обработки, дать поработать 10 секунд и остановить. Данные накапливаются в течение некоторого времени по триггеру в так называемый `mini-batch` и обрабатываются. Затем обновления добавляются в \"бесконечный\" DataFrame. Режим вывода может быть:\n",
    "- `update` - выводить только обновленные строки\n",
    "- `complete` - DataFrame полностью\n",
    "- `append` - новые строки\n",
    "\n",
    "Не все эти режимы доступны, зависит от применяемых операций обработки DataFrame. Результат будет выводиться в консоль. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+---------+-----+\n",
      "|timestamp|value|\n",
      "+---------+-----+\n",
      "+---------+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------------+-----+\n",
      "|timestamp              |value|\n",
      "+-----------------------+-----+\n",
      "|2025-11-12 12:12:12.111|0    |\n",
      "|2025-11-12 12:12:12.511|4    |\n",
      "|2025-11-12 12:12:12.911|8    |\n",
      "|2025-11-12 12:12:13.311|12   |\n",
      "|2025-11-12 12:12:13.711|16   |\n",
      "|2025-11-12 12:12:14.111|20   |\n",
      "|2025-11-12 12:12:14.511|24   |\n",
      "|2025-11-12 12:12:14.911|28   |\n",
      "|2025-11-12 12:12:15.311|32   |\n",
      "|2025-11-12 12:12:15.711|36   |\n",
      "|2025-11-12 12:12:16.111|40   |\n",
      "|2025-11-12 12:12:16.511|44   |\n",
      "|2025-11-12 12:12:16.911|48   |\n",
      "|2025-11-12 12:12:17.311|52   |\n",
      "|2025-11-12 12:12:17.711|56   |\n",
      "|2025-11-12 12:12:18.111|60   |\n",
      "|2025-11-12 12:12:18.511|64   |\n",
      "|2025-11-12 12:12:18.911|68   |\n",
      "|2025-11-12 12:12:19.311|72   |\n",
      "|2025-11-12 12:12:19.711|76   |\n",
      "+-----------------------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека [dbldatagen](https://github.com/databrickslabs/dbldatagen) позволяет, для тестовых целей, генерировать DataFrame с заданной схемой и случайным содержимом. Создадим DataFrame с одной колонкой, в которой может быть одно из пяти заданных слов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- word: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# описываем данные, которые будут генерироваться\n",
    "ds = dg.DataGenerator(spark, name=\"Words\", rows=20, partitions=1) \\\n",
    "      .withColumn(\"word\", StringType(), values=[\"hello\", \"world\", \"ok\", \"no\", \"yes\"], weights=[1, 1, 2, 2, 2])\n",
    "\n",
    "# создаем Streaming DataFrame\n",
    "df = ds.build(withStreaming=True, options={'rowsPerSecond': 3})\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+\n",
      "|word|\n",
      "+----+\n",
      "+----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+\n",
      "|word |\n",
      "+-----+\n",
      "|hello|\n",
      "|world|\n",
      "|ok   |\n",
      "|ok   |\n",
      "|no   |\n",
      "|no   |\n",
      "|yes  |\n",
      "|yes  |\n",
      "|hello|\n",
      "+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----+\n",
      "|word |\n",
      "+-----+\n",
      "|world|\n",
      "|ok   |\n",
      "|ok   |\n",
      "|no   |\n",
      "|no   |\n",
      "|yes  |\n",
      "+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----+\n",
      "|word |\n",
      "+-----+\n",
      "|yes  |\n",
      "|hello|\n",
      "|world|\n",
      "+-----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+----+\n",
      "|word|\n",
      "+----+\n",
      "|ok  |\n",
      "|ok  |\n",
      "|no  |\n",
      "+----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+----+\n",
      "|word|\n",
      "+----+\n",
      "|no  |\n",
      "|yes |\n",
      "|yes |\n",
      "+----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-----+\n",
      "|word |\n",
      "+-----+\n",
      "|hello|\n",
      "|world|\n",
      "|ok   |\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(10)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь можно описать преобразования (подсчет слов) и выводить текущую статистику в консоль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|word |count|\n",
      "+-----+-----+\n",
      "|hello|12   |\n",
      "|ok   |24   |\n",
      "|no   |24   |\n",
      "|world|12   |\n",
      "|yes  |24   |\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|word |count|\n",
      "+-----+-----+\n",
      "|hello|24   |\n",
      "|ok   |48   |\n",
      "|no   |48   |\n",
      "|world|24   |\n",
      "|yes  |48   |\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|word |count|\n",
      "+-----+-----+\n",
      "|hello|35   |\n",
      "|ok   |70   |\n",
      "|no   |70   |\n",
      "|world|35   |\n",
      "|yes  |69   |\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:==============================>                       (112 + 4) / 200]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      1\u001b[39m df = df.groupBy(\u001b[33m\"\u001b[39m\u001b[33mword\u001b[39m\u001b[33m\"\u001b[39m).count()\n\u001b[32m      3\u001b[39m query = df \\\n\u001b[32m      4\u001b[39m     .writeStream \\\n\u001b[32m      5\u001b[39m     .outputMode(\u001b[33m\"\u001b[39m\u001b[33mcomplete\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      6\u001b[39m     .format(\u001b[33m\"\u001b[39m\u001b[33mconsole\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      7\u001b[39m     .option(\u001b[33m\"\u001b[39m\u001b[33mtruncate\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      8\u001b[39m     .start()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m140\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m query.stop()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|word |count|\n",
      "+-----+-----+\n",
      "|hello|45   |\n",
      "|ok   |88   |\n",
      "|no   |88   |\n",
      "|world|45   |\n",
      "|yes  |88   |\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|word |count|\n",
      "+-----+-----+\n",
      "|hello|53   |\n",
      "|ok   |106  |\n",
      "|no   |104  |\n",
      "|world|53   |\n",
      "|yes  |104  |\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|word |count|\n",
      "+-----+-----+\n",
      "|hello|61   |\n",
      "|ok   |121  |\n",
      "|no   |120  |\n",
      "|world|61   |\n",
      "|yes  |120  |\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|word |count|\n",
      "+-----+-----+\n",
      "|hello|66   |\n",
      "|ok   |130  |\n",
      "|no   |130  |\n",
      "|world|66   |\n",
      "|yes  |130  |\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=====================================================>(198 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|word |count|\n",
      "+-----+-----+\n",
      "|hello|71   |\n",
      "|ok   |140  |\n",
      "|no   |140  |\n",
      "|world|70   |\n",
      "|yes  |140  |\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=======================>                               (87 + 4) / 200]\r"
     ]
    }
   ],
   "source": [
    "\n",
    "df = df.groupBy(\"word\").count()\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "\n",
    "time.sleep(140)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проэмулируем получение данных от трех IoT-устройств"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: timestamp (nullable = false)\n",
      " |-- sensor: string (nullable = false)\n",
      " |-- value: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds = dg.DataGenerator(spark, name=\"IOT\", rows=5000, partitions=1) \\\n",
    "      .withColumn(\"time\", \"timestamp\", expr=\"now()\") \\\n",
    "      .withColumn(\"sensor\", StringType(), values=[\"sensor_1\", \"sensor_2\", \"sensor_3\"]) \\\n",
    "      .withColumn(\"value\", \"integer\", minValue=0, maxValue=10, random=True)\n",
    "\n",
    "\n",
    "df = ds.build(withStreaming=True, options={'rowsPerSecond': 10})\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----------+\n",
      "|sensor|avg(value)|\n",
      "+------+----------+\n",
      "+------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:=====================================================>(198 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------+-----------------+\n",
      "|sensor  |avg(value)       |\n",
      "+--------+-----------------+\n",
      "|sensor_1|4.380952380952381|\n",
      "|sensor_2|5.445783132530121|\n",
      "|sensor_3|5.180722891566265|\n",
      "+--------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 14:51:52 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=false]] is aborting.\n",
      "25/11/19 14:51:52 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=20, truncate=false]] aborted.\n",
      "[Stage 13:=====================================================>(198 + 2) / 200]\r"
     ]
    }
   ],
   "source": [
    "df = ds.build(withStreaming=True, options={'rowsPerSecond': 10})\n",
    "\n",
    "df = df.groupBy(\"sensor\").avg(\"value\")\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(70)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно считать статистику по \"окнам\", которые образуются заданными временными интервалами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+------+----------+\n",
      "|window|sensor|avg(value)|\n",
      "+------+------+----------+\n",
      "+------+------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------------------------------------+--------+------------------+\n",
      "|window                                    |sensor  |avg(value)        |\n",
      "+------------------------------------------+--------+------------------+\n",
      "|{2025-11-19 14:54:20, 2025-11-19 14:54:30}|sensor_3|4.7631578947368425|\n",
      "|{2025-11-19 14:54:20, 2025-11-19 14:54:30}|sensor_1|5.077922077922078 |\n",
      "|{2025-11-19 14:54:20, 2025-11-19 14:54:30}|sensor_2|4.1558441558441555|\n",
      "+------------------------------------------+--------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------------------------------+--------+-----------------+\n",
      "|window                                    |sensor  |avg(value)       |\n",
      "+------------------------------------------+--------+-----------------+\n",
      "|{2025-11-19 14:54:40, 2025-11-19 14:54:50}|sensor_1|4.940298507462686|\n",
      "|{2025-11-19 14:54:40, 2025-11-19 14:54:50}|sensor_3|5.537313432835821|\n",
      "|{2025-11-19 14:54:40, 2025-11-19 14:54:50}|sensor_2|5.106060606060606|\n",
      "+------------------------------------------+--------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/19 14:55:16 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=false]] is aborting.\n",
      "25/11/19 14:55:16 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 3, writer: ConsoleWriter[numRows=20, truncate=false]] aborted.\n",
      "25/11/19 14:55:16 ERROR Utils: Aborting task\n",
      "org.apache.spark.SparkException: [CANNOT_WRITE_STATE_STORE.CANNOT_COMMIT] Error writing state store files for provider HDFSStateStore[id=(op=0,part=26),dir=file:/tmp/temporary-34bf4bd1-09de-41c4-bcec-c2624c6c72ed/state/0/26]. Cannot perform commit during state checkpoint. SQLSTATE: 58030\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToCommitStateFileError(QueryExecutionErrors.scala:2206)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:182)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$3.$anonfun$close$4(statefulOperators.scala:834)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:481)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:671)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$3.close(statefulOperators.scala:834)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask$IteratorWithMetrics.hasNext(WriteToDataSourceV2Exec.scala:545)\n",
      "\tat org.apache.spark.sql.connector.write.DataWriter.writeAll(DataWriter.java:107)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PackedRowDataWriter.writeAll(PackedRowWriterFactory.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.write(WriteToDataSourceV2Exec.scala:587)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:483)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:535)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:466)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:584)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:427)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.io.FileNotFoundException: File file:/tmp/temporary-34bf4bd1-09de-41c4-bcec-c2624c6c72ed/state/0/26 does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1713)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:211)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:896)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:807)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:512)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:807)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1044)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:376)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:156)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:190)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:447)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:174)\n",
      "\t... 35 more\n",
      "25/11/19 14:55:16 ERROR DataWritingSparkTask: Aborting commit for partition 26 (task 1900, attempt 0, stage 21.0)\n",
      "25/11/19 14:55:16 ERROR DataWritingSparkTask: Aborted commit for partition 26 (task 1900, attempt 0, stage 21.0)\n",
      "[Stage 21:======>                                                (25 + 2) / 200]\r"
     ]
    }
   ],
   "source": [
    "df = ds.build(withStreaming=True, options={'rowsPerSecond': 10})\n",
    "\n",
    "windowed_df = df\\\n",
    "    .groupBy(\n",
    "        window(df.time, \"10 seconds\"),\n",
    "        df.sensor\n",
    "    ).avg(\"value\")\n",
    "\n",
    "\n",
    "query = windowed_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(70)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Окна могут быть накладываться друг на друга или быть привязаны к какому-то полю события (\"сессионные\")\n",
    ".\n",
    "![](https://spark.apache.org/docs/latest/img/structured-streaming-time-window-types.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пересекающиеся - 10 секунд каждые 5.\n",
    "windowed_df = df\\\n",
    "   .groupBy(\n",
    "        window(df.time, \"10 seconds\", \"5 seconds\"),\n",
    "        df.sensor) \\\n",
    "    .avg(\"value\")\n",
    "\n",
    "# Разный размер окна в зависимости от датчика\n",
    "sw = session_window(df.time, \\\n",
    "    F.when(df.sensor == \"sensor_3\", \"5 seconds\") \\\n",
    "     .when(df.sensor == \"sensor_2\", \"10 seconds\") \\\n",
    "     .otherwise(\"5 seconds\"))\n",
    "\n",
    "windowed_df = df\\\n",
    "    .groupBy(\n",
    "        sw,\n",
    "        df.sensor) \\\n",
    "    .avg(\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Событие может быть создано намного раньше времени физической обработки Spark'ом. Например, это может быть связано с высокой нагрузкой или проблемой с сетью. Обработка такого события приведет к изменению исторических данных. Для того, чтобы избежать подобного, можно воспользоваться \"watermark\", указав максимальное время между значением поля времени и временем обработки события. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "windowed_df = df\\\n",
    "    .withWatermark(\"time\", \"15 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(df.time, \"10 seconds\"),\n",
    "        df.sensor\n",
    "    ).avg(\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Spark Streaming` поддерживает операции вида `join` между двумя датафреймами. Причем датафрейм может быть статическим или динамическим. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------+----+-----+-----------+\n",
      "|sensor|time|value|description|\n",
      "+------+----+-----+-----------+\n",
      "+------+----+-----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor  |time                      |value|description|\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor_1|2024-10-10 10:27:09.879887|10   |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:09.879887|2    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:09.879887|0    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:09.879887|5    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:09.879887|9    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:09.879887|8    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:09.879887|5    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:09.879887|8    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:09.879887|8    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:09.879887|8    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:09.879887|6    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:09.879887|4    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:09.879887|4    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:09.879887|6    |Sensor #1  |\n",
      "|sensor_2|2024-10-10 10:27:09.879887|9    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:09.879887|5    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:09.879887|3    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:09.879887|6    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:09.879887|3    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:09.879887|9    |Sensor #2  |\n",
      "+--------+--------------------------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor  |time                      |value|description|\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor_1|2024-10-10 10:27:12.599167|7    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:12.599167|4    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:12.599167|3    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:12.599167|9    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:12.599167|8    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:12.599167|5    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:12.599167|7    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:12.599167|9    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:12.599167|7    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:12.599167|8    |Sensor #1  |\n",
      "|sensor_2|2024-10-10 10:27:12.599167|5    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:12.599167|3    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:12.599167|8    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:12.599167|8    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:12.599167|6    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:12.599167|1    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:12.599167|7    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:12.599167|1    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:12.599167|10   |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:12.599167|7    |Sensor #2  |\n",
      "+--------+--------------------------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor  |time                      |value|description|\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor_1|2024-10-10 10:27:15.726165|3    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:15.726165|9    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:15.726165|10   |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:15.726165|9    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:15.726165|1    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:15.726165|7    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:15.726165|7    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:15.726165|1    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:15.726165|6    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:15.726165|9    |Sensor #1  |\n",
      "|sensor_2|2024-10-10 10:27:15.726165|7    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:15.726165|10   |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:15.726165|2    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:15.726165|7    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:15.726165|4    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:15.726165|9    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:15.726165|8    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:15.726165|9    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:15.726165|10   |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:15.726165|9    |Sensor #2  |\n",
      "+--------+--------------------------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor  |time                      |value|description|\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor_1|2024-10-10 10:27:17.939313|8    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:17.939313|6    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:17.939313|0    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:17.939313|6    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:17.939313|0    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:17.939313|9    |Sensor #1  |\n",
      "|sensor_2|2024-10-10 10:27:17.939313|4    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:17.939313|4    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:17.939313|7    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:17.939313|0    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:17.939313|4    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:17.939313|0    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:17.939313|10   |Sensor #2  |\n",
      "|sensor_3|2024-10-10 10:27:17.939313|3    |Sensor #3  |\n",
      "|sensor_3|2024-10-10 10:27:17.939313|6    |Sensor #3  |\n",
      "|sensor_3|2024-10-10 10:27:17.939313|0    |Sensor #3  |\n",
      "|sensor_3|2024-10-10 10:27:17.939313|8    |Sensor #3  |\n",
      "|sensor_3|2024-10-10 10:27:17.939313|8    |Sensor #3  |\n",
      "|sensor_3|2024-10-10 10:27:17.939313|10   |Sensor #3  |\n",
      "|sensor_3|2024-10-10 10:27:17.939313|1    |Sensor #3  |\n",
      "+--------+--------------------------+-----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 5\n",
      "-------------------------------------------\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor  |time                      |value|description|\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor_1|2024-10-10 10:27:20.322373|0    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:20.322373|10   |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:20.322373|3    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:20.322373|1    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:20.322373|1    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:20.322373|5    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:20.322373|5    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:20.322373|1    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:20.322373|3    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:20.322373|8    |Sensor #1  |\n",
      "|sensor_2|2024-10-10 10:27:20.322373|7    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:20.322373|3    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:20.322373|9    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:20.322373|5    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:20.322373|4    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:20.322373|8    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:20.322373|3    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:20.322373|7    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:20.322373|5    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:20.322373|7    |Sensor #2  |\n",
      "+--------+--------------------------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 6\n",
      "-------------------------------------------\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor  |time                      |value|description|\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor_1|2024-10-10 10:27:22.601582|6    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:22.601582|9    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:22.601582|7    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:22.601582|6    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:22.601582|8    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:22.601582|10   |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:22.601582|1    |Sensor #1  |\n",
      "|sensor_2|2024-10-10 10:27:22.601582|7    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:22.601582|2    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:22.601582|6    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:22.601582|0    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:22.601582|7    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:22.601582|2    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:22.601582|5    |Sensor #2  |\n",
      "|sensor_3|2024-10-10 10:27:22.601582|5    |Sensor #3  |\n",
      "|sensor_3|2024-10-10 10:27:22.601582|1    |Sensor #3  |\n",
      "|sensor_3|2024-10-10 10:27:22.601582|9    |Sensor #3  |\n",
      "|sensor_3|2024-10-10 10:27:22.601582|8    |Sensor #3  |\n",
      "|sensor_3|2024-10-10 10:27:22.601582|9    |Sensor #3  |\n",
      "|sensor_3|2024-10-10 10:27:22.601582|0    |Sensor #3  |\n",
      "+--------+--------------------------+-----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 7\n",
      "-------------------------------------------\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor  |time                      |value|description|\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor_1|2024-10-10 10:27:24.743627|6    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:24.743627|10   |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:24.743627|9    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:24.743627|7    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:24.743627|0    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:24.743627|9    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:24.743627|10   |Sensor #1  |\n",
      "|sensor_2|2024-10-10 10:27:24.743627|4    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:24.743627|6    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:24.743627|7    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:24.743627|5    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:24.743627|4    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:24.743627|3    |Sensor #2  |\n",
      "|sensor_3|2024-10-10 10:27:24.743627|2    |Sensor #3  |\n",
      "|sensor_3|2024-10-10 10:27:24.743627|9    |Sensor #3  |\n",
      "|sensor_3|2024-10-10 10:27:24.743627|2    |Sensor #3  |\n",
      "|sensor_3|2024-10-10 10:27:24.743627|8    |Sensor #3  |\n",
      "|sensor_3|2024-10-10 10:27:24.743627|8    |Sensor #3  |\n",
      "|sensor_3|2024-10-10 10:27:24.743627|1    |Sensor #3  |\n",
      "|sensor_3|2024-10-10 10:27:24.743627|3    |Sensor #3  |\n",
      "+--------+--------------------------+-----+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 8\n",
      "-------------------------------------------\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor  |time                      |value|description|\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor_1|2024-10-10 10:27:28.375509|10   |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:28.375509|9    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:28.375509|2    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:28.375509|2    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:28.375509|1    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:28.375509|9    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:28.375509|6    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:28.375509|1    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:28.375509|1    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:28.375509|4    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:28.375509|10   |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:28.375509|1    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:28.375509|0    |Sensor #1  |\n",
      "|sensor_2|2024-10-10 10:27:28.375509|2    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:28.375509|8    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:28.375509|2    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:28.375509|1    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:28.375509|1    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:28.375509|4    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:28.375509|4    |Sensor #2  |\n",
      "+--------+--------------------------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 9\n",
      "-------------------------------------------\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor  |time                      |value|description|\n",
      "+--------+--------------------------+-----+-----------+\n",
      "|sensor_1|2024-10-10 10:27:31.864305|7    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:31.864305|7    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:31.864305|6    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:31.864305|6    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:31.864305|9    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:31.864305|5    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:31.864305|8    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:31.864305|10   |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:31.864305|9    |Sensor #1  |\n",
      "|sensor_1|2024-10-10 10:27:31.864305|2    |Sensor #1  |\n",
      "|sensor_2|2024-10-10 10:27:31.864305|9    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:31.864305|8    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:31.864305|1    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:31.864305|0    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:31.864305|4    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:31.864305|10   |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:31.864305|7    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:31.864305|3    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:31.864305|7    |Sensor #2  |\n",
      "|sensor_2|2024-10-10 10:27:31.864305|9    |Sensor #2  |\n",
      "+--------+--------------------------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/10 10:27:35 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=false]] is aborting.\n",
      "24/10/10 10:27:35 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 10, writer: ConsoleWriter[numRows=20, truncate=false]] aborted.\n"
     ]
    }
   ],
   "source": [
    "df = ds.build(withStreaming=True, options={'rowsPerSecond': 10})\n",
    "\n",
    "descr_df = spark.createDataFrame([(\"sensor_1\", \"Sensor #1\"), (\"sensor_2\", \"Sensor #2\"), (\"sensor_3\", \"Sensor #3\")], [\"sensor\", \"description\"])\n",
    "\n",
    "res_df = df.join(descr_df, \"sensor\")\n",
    "\n",
    "query = res_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(30)\n",
    "\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ds.build(withStreaming=True, options={'rowsPerSecond': 10})\n",
    "\n",
    "windows_df_5 = df \\\n",
    "    .withWatermark(\"time\", \"30 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(\"time\", \"5 seconds\"),\n",
    "        \"sensor\") \\\n",
    "    .agg(F.avg(\"value\").alias(\"value\")) \\\n",
    "    .select(F.window_time(\"window\").alias(\"time\"), \"value\", \"sensor\")\n",
    "\n",
    "\n",
    "windows_df_10 = df \\\n",
    "    .withWatermark(\"time\", \"20 seconds\") \\\n",
    "    .groupBy(\n",
    "        window(\"time\", \"10 seconds\"),\n",
    "        \"sensor\") \\\n",
    "    .agg(F.avg(\"value\").alias(\"value\")) \\\n",
    "    .select(F.window_time(\"window\").alias(\"time\"), \"value\", \"sensor\")    \n",
    " \n",
    " \n",
    "res_df = windows_df_5.alias(\"df_5\").join(\n",
    "    windows_df_10.alias(\"df_10\"),\n",
    "    F.expr(\"\"\"\n",
    "        df_5.sensor = df_10.sensor  AND\n",
    "        df_5.time >= df_10.time AND\n",
    "        df_5.time <= df_10.time + interval 40 seconds\n",
    "        \"\"\")\n",
    ")\n",
    "\n",
    "query = res_df \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "time.sleep(50)\n",
    "\n",
    "query.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_sp (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
